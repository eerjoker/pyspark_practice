{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eerjoker/pyspark_practice/blob/main/pyspark_practice_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leo un csv y filtro\n",
        "\n"
      ],
      "metadata": {
        "id": "vswDQRGxoYHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "# filtro y muestro\n",
        "df.where(df['_c1'] > 40).select(df['_c0'], df['_c1']).show(5)"
      ],
      "metadata": {
        "id": "2wQ7tKbcoZMW",
        "outputId": "e1b86a05-b450-4e1f-cc0f-cac031fc9410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "|        _c0|      _c1|\n",
            "+-----------+---------+\n",
            "|-123.520000|41.010000|\n",
            "|-122.270000|41.230000|\n",
            "|-124.160000|41.920000|\n",
            "|-124.170000|41.800000|\n",
            "|-122.320000|41.310000|\n",
            "+-----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardar como parquet"
      ],
      "metadata": {
        "id": "WyBYCigpXDDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "# guardo parquet\n",
        "dir = '/content/sample_data/new_files'\n",
        "df.write.parquet(dir, mode=\"overwrite\")\n",
        "spark.read.parquet(dir).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7JXZXPXB35B",
        "outputId": "bd5f6437-974b-4ae8-925f-2344b13d97c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|        _c0|      _c1|               _c2|        _c3|           _c4|        _c5|       _c6|          _c7|               _c8|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000|441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000|599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000|603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000|261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000|138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000|170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000|659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000|331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000| 50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000|107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000|595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000|199.000000|     6.585100|     300000.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Arreglo headers y agrego columna date"
      ],
      "metadata": {
        "id": "fu1A3SuSXn8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.option(\"header\", True).csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "# nueva columna\n",
        "df = df.withColumn('fecha_hoy', functions.current_date())\n",
        "\n",
        "# guardo\n",
        "dir = '/content/sample_data/new_files'\n",
        "df.write.parquet(dir, mode=\"overwrite\")\n",
        "spark.read.parquet(dir).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHipLUEAXwRX",
        "outputId": "5618f5e3-40f4-4eee-f4e3-76a57ec127ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+----------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value| fecha_hoy|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+----------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|2025-04-30|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|2025-04-30|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|2025-04-30|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|2025-04-30|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000| 237.000000|     2.937500|      81700.000000|2025-04-30|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000| 204.000000|     1.663500|      67000.000000|2025-04-30|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000| 218.000000|     1.664100|      67000.000000|2025-04-30|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000| 441.000000|     3.225000|     166900.000000|2025-04-30|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000| 599.000000|     3.669600|     194400.000000|2025-04-30|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|2025-04-30|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|2025-04-30|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000| 138.000000|     2.416700|      58300.000000|2025-04-30|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|2025-04-30|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|2025-04-30|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|2025-04-30|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|2025-04-30|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000| 107.000000|     1.875000|     181300.000000|2025-04-30|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000| 595.000000|     2.717400|     137500.000000|2025-04-30|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|2025-04-30|\n",
            "|-122.590000|38.010000|         35.000000|8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|2025-04-30|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select, fecha ayer y ultima fecha habil"
      ],
      "metadata": {
        "id": "sFRwbIai296y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.option(\"header\", True).csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "\n",
        "# nuevas columnas\n",
        "df = df.withColumn('fecha_hoy', functions.current_date())\n",
        "df = df.withColumn('fecha_ayer',  functions.date_add(df['fecha_hoy'], -1)) \\\n",
        "  .withColumn('ultima_fecha_habil', functions.when(functions.dayofweek(df['fecha_hoy']) == 1, functions.date_add(df['fecha_hoy'], -2))\n",
        "                                  .when(functions.dayofweek(df['fecha_hoy']) == 2, functions.date_add(df['fecha_hoy'], -3))\n",
        "                                  .otherwise(functions.date_add(df['fecha_hoy'], -1)))\n",
        "\n",
        "#creo tabla temp\n",
        "df.createOrReplaceTempView('calif_housing_table')\n",
        "\n",
        "#select\n",
        "spark.sql(\"select * from calif_housing_table where latitude < 35\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Txajet8NwH",
        "outputId": "d1f607d5-21cf-4f1e-eb1e-95633a437d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+----------+----------+------------------+\n",
            "|  longitude| latitude|housing_median_age| total_rooms|total_bedrooms| population| households|median_income|median_house_value| fecha_hoy|fecha_ayer|ultima_fecha_habil|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+----------+----------+------------------+\n",
            "|-118.300000|34.260000|         43.000000| 1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-117.810000|33.780000|         27.000000| 3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.360000|33.820000|         28.000000|   67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.020000|34.080000|         31.000000| 2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.240000|33.980000|         45.000000|  972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-117.030000|32.970000|         16.000000| 3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-117.970000|33.730000|         27.000000| 2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-117.990000|33.810000|         42.000000|  161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.880000|34.210000|         26.000000| 1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.160000|34.070000|         47.000000| 2994.000000|    543.000000|1651.000000| 561.000000|     3.864400|     241500.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-117.280000|33.280000|         13.000000| 6131.000000|   1040.000000|4049.000000| 940.000000|     3.815600|     150700.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.030000|34.160000|         36.000000| 1401.000000|    218.000000| 667.000000| 225.000000|     7.161500|     484700.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.390000|33.990000|         32.000000| 2612.000000|    418.000000|1030.000000| 402.000000|     6.603000|     369200.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.450000|34.070000|         19.000000| 4845.000000|   1609.000000|3751.000000|1539.000000|     1.583000|     350000.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.480000|34.010000|         30.000000| 3078.000000|    954.000000|1561.000000| 901.000000|     3.485200|     425000.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.300000|33.910000|         34.000000| 1617.000000|    493.000000|1530.000000| 500.000000|     2.618200|     172600.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.080000|34.550000|          5.000000|16181.000000|   2971.000000|8152.000000|2651.000000|     4.523700|     141800.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.320000|33.940000|         38.000000| 1067.000000|    170.000000| 499.000000| 169.000000|     4.638900|     183800.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.110000|34.000000|         33.000000| 2886.000000|    726.000000|2650.000000| 728.000000|     2.625000|     178700.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "|-118.020000|33.920000|         34.000000| 1478.000000|    251.000000| 956.000000| 277.000000|     5.523800|     185300.000000|2025-05-01|2025-04-30|        2025-04-30|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+----------+----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Otra forma con fecha ayer en sql"
      ],
      "metadata": {
        "id": "eCKh3XesnPf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.option(\"header\", True).csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "# nuevas columnas\n",
        "df = df.withColumn('fecha_hoy', functions.current_date())\n",
        "\n",
        "#creo tabla temp\n",
        "df.createOrReplaceTempView('calif_housing_table')\n",
        "\n",
        "#select\n",
        "spark.sql(\"select fecha_hoy, DATEADD(fecha_hoy, -1) fecha_ayer from calif_housing_table\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaiSShW4nRX8",
        "outputId": "755cff2c-cbb9-4031-d03d-d2458aae03f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "| fecha_hoy|fecha_ayer|\n",
            "+----------+----------+\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "|2025-05-01|2025-04-30|\n",
            "+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from json to parquet"
      ],
      "metadata": {
        "id": "nPLRo3CVrXaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.option(\"multiLine\", True).json('/content/sample_data/anscombe.json')\n",
        "\n",
        "# guardo\n",
        "dir = '/content/sample_data/new_files'\n",
        "df.write.parquet(dir, mode=\"overwrite\")\n",
        "spark.read.parquet(dir).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvorIvY-u908",
        "outputId": "726b96e3-1c8f-4c28-ab98-f7d1498444f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "|    II|12.0| 9.13|           NULL|\n",
            "|    II| 7.0| 7.26|           NULL|\n",
            "|    II| 5.0| 4.74|           NULL|\n",
            "|   III|10.0| 7.46|           NULL|\n",
            "|   III| 8.0| 6.77|           NULL|\n",
            "|   III|13.0|12.74|           NULL|\n",
            "|   III| 9.0| 7.11|           NULL|\n",
            "|   III|11.0| 7.81|           NULL|\n",
            "|   III|14.0| 8.84|           NULL|\n",
            "|   III| 6.0| 6.08|           NULL|\n",
            "|   III| 4.0| 5.39|           NULL|\n",
            "|   III|12.0| 8.15|           NULL|\n",
            "|   III| 7.0| 6.42|           NULL|\n",
            "|   III| 5.0| 5.73|           NULL|\n",
            "|    IV| 8.0| 6.58|           NULL|\n",
            "|    IV| 8.0| 5.76|           NULL|\n",
            "|    IV| 8.0| 7.71|           NULL|\n",
            "|    IV| 8.0| 8.84|           NULL|\n",
            "|    IV| 8.0| 8.47|           NULL|\n",
            "|    IV| 8.0| 7.04|           NULL|\n",
            "|    IV| 8.0| 5.25|           NULL|\n",
            "|    IV|19.0| 12.5|           NULL|\n",
            "|    IV| 8.0| 5.56|           NULL|\n",
            "|    IV| 8.0| 7.91|           NULL|\n",
            "|    IV| 8.0| 6.89|           NULL|\n",
            "|  NULL|NULL| NULL|              ]|\n",
            "+------+----+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#from fixed length to parquet"
      ],
      "metadata": {
        "id": "uLIGxdk0Bvhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "schema = [\n",
        "    (0, 8),\n",
        "    (8, 16),\n",
        "    (24, 16),\n",
        "    (40, 12),\n",
        "    (52, 14),\n",
        "    (66, 16),\n",
        "    (82, 7)\n",
        "]\n",
        "\n",
        "spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "df = spark.read.text('/content/sample_data/credit_balance_01.fw')\n",
        "\n",
        "# obtengo los headers\n",
        "headers = str(df.first()[0])\n",
        "\n",
        "# completo cada columna separando segun el schema\n",
        "for colinfo in schema:\n",
        "  df = df.withColumn(headers[colinfo[0] : colinfo[1] + colinfo[0]], \\\n",
        "                     df.value.substr(colinfo[0], colinfo[1]))\n",
        "\n",
        "# limpio la fila de headers y la columna de la primer lectura\n",
        "df = df.filter(~functions.col('value').contains(headers)) \\\n",
        "       .drop('value')\n",
        "\n",
        "# guardo\n",
        "dir = '/content/sample_data/new_files'\n",
        "df.write.parquet(dir, mode=\"overwrite\")\n",
        "spark.read.parquet(dir).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plifEGp-Bt5M",
        "outputId": "135aed47-ce59-4cad-ce2c-963cda014605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "|Account |LastName        |FirstName       |Balance     |CreditLimit   |AccountCreated  |Rating |\n",
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "|101     | Reeves         | Keanu          | 9315.45    | 10000.00     | 1/17/1998      | A     |\n",
            "|312     | Butler         | Gerard         | 90.00      | 1000.00      | 8/6/2003       | B     |\n",
            "|868     | Hewitt         | Jennifer Love  | 0          | 17000.00     | 5/25/1985      | B     |\n",
            "|761     | Pinkett-Smith  | Jada           | 49654.87   | 100000.00    | 12/5/2006      | A     |\n",
            "|317     | Murray         | Bill           | 789.65     | 5000.00      | 2/5/2007       | C     |\n",
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from fixed length to parquet with classes and casting"
      ],
      "metadata": {
        "id": "xu1KSTL325Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, types, functions\n",
        "\n",
        "class FixedLengthToParquet:\n",
        "  def __init__(self, schema, inputFileName, outputFileName):\n",
        "    self.schema = schema\n",
        "    self.inputFileName = inputFileName\n",
        "    self.outputFileName = outputFileName\n",
        "\n",
        "  def run(self):\n",
        "    spark = SparkSession.builder.appName(\"main_app\").getOrCreate()\n",
        "    df = spark.read.text(self.inputFileName)\n",
        "\n",
        "    # obtengo los headers\n",
        "    headers = str(df.first()[0])\n",
        "\n",
        "    # completo cada columna separando segun el schema\n",
        "    for colinfo in schema:\n",
        "      df = df.withColumn(headers[colinfo[0] : colinfo[1] + colinfo[0]], \\\n",
        "        functions.to_date(functions.trim(df.value.substr(colinfo[0], colinfo[1])), \"M/d/yyyy\") \\\n",
        "          if isinstance(colinfo[2], types.DateType) \\\n",
        "          else df.value.substr(colinfo[0], colinfo[1]).cast(colinfo[2]))\n",
        "\n",
        "    # limpio la fila de headers y la columna de la primer lectura\n",
        "    df = df.filter(~functions.col('value').contains(headers)) \\\n",
        "           .drop('value')\n",
        "\n",
        "    # guardo\n",
        "    dir = self.outputFileName\n",
        "    df.write.parquet(dir, mode=\"overwrite\")\n",
        "    spark.read.parquet(dir).show()\n",
        "\n",
        "schema = [\n",
        "    (0, 8, types.IntegerType()),\n",
        "    (8, 16, types.StringType()),\n",
        "    (24, 16, types.StringType()),\n",
        "    (40, 12, types.DoubleType()),\n",
        "    (52, 14, types.DoubleType()),\n",
        "    (66, 16, types.DateType()),\n",
        "    (82, 7, types.StringType())\n",
        "]\n",
        "\n",
        "# main\n",
        "FixedLengthToParquet(schema, \\\n",
        "                     '/content/sample_data/credit_balance_01.fw', \\\n",
        "                     '/content/sample_data/new_files') \\\n",
        "                     .run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNU2mZPG3DGc",
        "outputId": "5bb57547-488b-4eb9-bf4d-870b3c717a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "|Account |LastName        |FirstName       |Balance     |CreditLimit   |AccountCreated  |Rating |\n",
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "|     101| Reeves         | Keanu          |     9315.45|       10000.0|      1998-01-17| A     |\n",
            "|     312| Butler         | Gerard         |        90.0|        1000.0|      2003-08-06| B     |\n",
            "|     868| Hewitt         | Jennifer Love  |         0.0|       17000.0|      1985-05-25| B     |\n",
            "|     761| Pinkett-Smith  | Jada           |    49654.87|      100000.0|      2006-12-05| A     |\n",
            "|     317| Murray         | Bill           |      789.65|        5000.0|      2007-02-05| C     |\n",
            "+--------+----------------+----------------+------------+--------------+----------------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}